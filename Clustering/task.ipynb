{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(chunk,n_cluster):\n",
    "    n = len(chunk)\n",
    "\n",
    "    # RS\n",
    "    temp = []\n",
    "    chunk_X = [p['dimensions'] for p in chunk]\n",
    "    RS_kmeans = KMeans(n_clusters=n_cluster*5, random_state=0).fit(chunk_X)\n",
    "    RS_clusters = [clst_id for clst_id,count in Counter(RS_kmeans.labels_).items() if count==1]\n",
    "    for id,point in enumerate(chunk):\n",
    "        if RS_kmeans.labels_[id] in RS_clusters:\n",
    "            chunk_X.pop(id)\n",
    "            temp.append(point)\n",
    "    \n",
    "    # DS\n",
    "    DS = [Cluster() for i in range(n_cluster)]\n",
    "    DS_kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(chunk_X)\n",
    "    for id,point in enumerate(chunk_X):\n",
    "        clst_id = DS_kmeans.labels_[id]\n",
    "        DS[clst_id].update(point)\n",
    "\n",
    "    # CS\n",
    "    CS = []\n",
    "    RS = []\n",
    "    if len(temp) >= n_cluster*5:\n",
    "        temp_X = [p['dimensions'] for p in temp]\n",
    "        CS_kmeans = KMeans(n_clusters=n_cluster*5, random_state=0).fit(temp_X)\n",
    "        CS_clusters = [clst_id for clst_id,count in Counter(CS_kmeans.labels_).items() if count!=1]\n",
    "        CS = [Cluster() for i in CS_clusters]\n",
    "        for id,point in enumerate(temp):\n",
    "            p_label = CS_kmeans.labels_[id]\n",
    "            if p_label in CS_clusters:\n",
    "                CS[CS_clusters.index(p_label)].update(point['dimensions'])\n",
    "            else:\n",
    "                RS.append(point) \n",
    "    else:\n",
    "        RS = temp\n",
    "    return DS,CS,RS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self) -> None:\n",
    "        self.N = 0\n",
    "        self.SUM = 0\n",
    "        self.SUMSQ = 0\n",
    "\n",
    "    def centroid(self):\n",
    "        return self.SUM/self.N\n",
    "    \n",
    "    def std(self):\n",
    "        return np.sqrt((self.SUMSQ/self.N)-np.square(self.centroid()))\n",
    "\n",
    "    def update(self,x):\n",
    "        self.N += 1\n",
    "        self.SUM += x\n",
    "        self.SUMSQ += np.square(x)\n",
    "\n",
    "    def combine(self,cluster):\n",
    "        self.N += cluster.N\n",
    "        self.SUM += cluster.SUM\n",
    "        self.SUMSQ += cluster.SUMSQ\n",
    "\n",
    "    def mahalanobis_distance(self,x):\n",
    "        c = self.centroid()\n",
    "        std = self.std()\n",
    "        return np.sqrt(sum(np.square((x-c)/std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_mapping(chunk,n_cluster):\n",
    "    X = [p['dimensions'] for p in chunk]\n",
    "    kmeans = KMeans(n_clusters=n_cluster*5, random_state=0).fit(X)\n",
    "    result = {i:[] for i in range(n_cluster*5)}\n",
    "    for clst_id,point in zip(kmeans.labels_,chunk):\n",
    "        result[clst_id].append(point)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(chunk,n_cluster):\n",
    "    n = len(chunk)\n",
    "    assigned = []\n",
    "    unassigned = []\n",
    "    for clst_id,cluster in create_cluster_mapping(chunk,n_cluster).items():\n",
    "        if len(cluster)==1:\n",
    "            unassigned += cluster\n",
    "        else:\n",
    "            assigned += cluster\n",
    "\n",
    "    # DS\n",
    "    DS = [Cluster() for i in range(n_cluster)]\n",
    "    assigned_X = [p['dimensions'] for p in assigned]\n",
    "    DS_kmeans = KMeans(n_clusters=n_cluster, random_state=0).fit(assigned_X)\n",
    "    for clst_id,x in zip(DS_kmeans.labels_,assigned_X):\n",
    "        DS[clst_id].update(x)\n",
    "\n",
    "    # CS RS\n",
    "    CS = []\n",
    "    RS = []\n",
    "    if len(unassigned) >= n_cluster*5:\n",
    "        for clst_id,cluster in create_cluster_mapping(unassigned,n_cluster).items():\n",
    "            if len(cluster)==1:\n",
    "                RS += cluster\n",
    "            else:\n",
    "                new_cluster = Cluster()\n",
    "                for point in cluster:\n",
    "                    new_cluster.update(point[\"dimensions\"])\n",
    "                CS.append(new_cluster)\n",
    "    else:\n",
    "        RS += unassigned     \n",
    "    return DS,CS,RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  intermediate_results(DS,CS,RS):\n",
    "    return (\n",
    "        sum([c.N for c in DS]),\n",
    "        len(CS),\n",
    "        0+sum([c.N for c in CS]),\n",
    "        len(RS)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clst_wise_mdist(clst1,clst2):\n",
    "    c1 = clst1.centroid()\n",
    "    c2 = clst2.centroid()\n",
    "    if np.sum(np.square(clst1.std())) < np.sum(np.square(clst2.std())):\n",
    "        std = clst1.std()\n",
    "    else:\n",
    "        std = clst2.std()\n",
    "    return np.sqrt(sum(np.square((c1-c2)/std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_path, n_cluster, output_path):\n",
    "    data = []\n",
    "    with open(input_path) as file:\n",
    "        reader = csv.reader(file, delimiter=',')\n",
    "        for line in reader:\n",
    "            data.append({'id':line[0],'cluster_gt':line[1],'dimensions':np.array([float(i) for i in line[2:]])})\n",
    "    random.shuffle(data)\n",
    "    int_result = []\n",
    "    thres = 2*np.sqrt(len(data[0]['dimensions']))\n",
    "    data_chunks = np.array_split(data,5)\n",
    "    DS,CS,RS = initialize(data_chunks[0],n_cluster)\n",
    "    int_result.append(intermediate_results(DS,CS,RS))\n",
    "    count = len(data_chunks[0])\n",
    "    for i in range(1,5):\n",
    "        temp = []\n",
    "        chunk = data_chunks[i]\n",
    "        for point in chunk:\n",
    "            x = point[\"dimensions\"]\n",
    "            assigned = False\n",
    "            for cluster in DS+CS:\n",
    "                if cluster.mahalanobis_distance(x) < thres:\n",
    "                    cluster.update(x)\n",
    "                    assigned = True\n",
    "                    break\n",
    "            if not assigned:\n",
    "                temp.append(point)\n",
    "        temp += RS\n",
    "\n",
    "        if len(temp) >= n_cluster*5:\n",
    "            RS = []\n",
    "            for clst_id,cluster in create_cluster_mapping(temp,n_cluster).items():\n",
    "                if len(cluster)==1:\n",
    "                    RS += cluster\n",
    "                else:\n",
    "                    new_cluster = Cluster()\n",
    "                    if cluster[0] == cluster[1]:\n",
    "                        pass\n",
    "                    for point in cluster:\n",
    "                        new_cluster.update(point[\"dimensions\"])\n",
    "                    CS.append(new_cluster)\n",
    "        else:\n",
    "            RS = temp\n",
    "        \n",
    "        merge_complete = False\n",
    "        while not merge_complete:\n",
    "            merge_complete = True\n",
    "            for clst1_id,clst2_id in combinations(range(len(CS)),2):\n",
    "                if clst_wise_mdist(CS[clst1_id],CS[clst2_id]) < thres:\n",
    "                    CS[clst1_id] = CS[clst1_id].combine(CS[clst2_id])\n",
    "                    CS.pop(clst2_id)\n",
    "                    merge_complete = False\n",
    "                    break\n",
    "        count+= len(chunk)\n",
    "\n",
    "        if i==4:\n",
    "            while True:  \n",
    "                try:\n",
    "                    for cs_id,cs_cluster in enumerate(CS):\n",
    "                        for ds_id,ds_cluster in enumerate(DS):\n",
    "                            if clst_wise_mdist(cs_cluster,ds_cluster) < thres:\n",
    "                                raise StopIteration\n",
    "                except StopIteration:\n",
    "                    DS[ds_id].combine(cs_cluster)\n",
    "                    CS.pop(cs_id)\n",
    "                    continue\n",
    "                break\n",
    "        int_result.append(intermediate_results(DS,CS,RS))\n",
    "\n",
    "    result = []\n",
    "    for point in data:\n",
    "        for clst_id,cluster in enumerate(DS):\n",
    "            if cluster.mahalanobis_distance(point['dimensions']) < thres:\n",
    "                    label = clst_id\n",
    "                    break\n",
    "        else:\n",
    "            label = -1\n",
    "        result.append((int(point['id']),label))\n",
    "\n",
    "    result.sort(key=lambda x: x[0])\n",
    "    with open(output_path,'w') as file:\n",
    "        file.write(\"The intermediate results:\\n\")\n",
    "        for i,record in enumerate(int_result):\n",
    "            file.write(f'Round {i+1}: {record[0]},{record[1]},{record[2]},{record[3]}'+'\\n')\n",
    "        file.write('\\n')\n",
    "        file.write('The clustering results:\\n')\n",
    "        for id,label in result:\n",
    "            file.write(f'{id},{label}\\n')\n",
    "    \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path, n_cluster, output_path = ('../resource/asnlib/publicdata/hw6_clustering.txt', 9, 'result.txt')\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64463\n"
     ]
    }
   ],
   "source": [
    "result = main(input_path, n_cluster, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('newenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87ef2c4983d124858e7bc65373e634017d4d0da0a01665b9091a743c5bc24cc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
